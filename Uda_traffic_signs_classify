# Load pickled data
import pickle

# TODO: Fill this in based on where you saved the training and testing data

training_file = r'C:\Users\Administrator\Desktop\Uda-Autopilot\CarND-Traffic-Sign-Classifier-Project-master\traffic-signs-data\train.p'
validation_file = r'C:\Users\Administrator\Desktop\Uda-Autopilot\CarND-Traffic-Sign-Classifier-Project-master\traffic-signs-data\valid.p'
testing_file = r'C:\Users\Administrator\Desktop\Uda-Autopilot\CarND-Traffic-Sign-Classifier-Project-master\traffic-signs-data\test.p'

with open(training_file, mode='rb') as f:
    train = pickle.load(f)
with open(validation_file, mode='rb') as f:
    valid = pickle.load(f)
with open(testing_file, mode='rb') as f:
    test = pickle.load(f)

X_train_o, y_train = train['features'], train['labels']
X_valid_o, y_valid = valid['features'], valid['labels']
X_test_o, y_test = test['features'], test['labels']

### Replace each question mark with the appropriate value.
### Use python, pandas or numpy methods rather than hard coding the results
import numpy as np

# TODO: Number of training examples
n_train = len(X_train_o)

# TODO: Number of validation examples
n_validation = len(X_valid_o)

# TODO: Number of testing examples.
n_test = len(X_test_o)

# TODO: What's the shape of an traffic sign image?
temp = np.array(X_train_o[0])
image_shape = temp.shape

# TODO: How many unique classes/labels there are in the dataset.
temp = set(y_train)
n_classes = len(temp)

print("Number of training examples =", n_train)
print("Number of validation examples =", n_validation)
print("Number of testing examples =", n_test)
print("Image data shape =", image_shape)
print("Number of classes =", n_classes)

import matplotlib.pyplot as plt
# Visualizations will be shown in the notebook.
k = 50
def show(k):
    picture_1, label_1 = X_train[k].squeeze(), y_train[k]
    print('The lablel is: ',label_1)
    plt.imshow(picture_1)
    plt.show()

import cv2
### Gray
X_train_G, X_valid_G, X_test_G = [], [], []
for i in range(n_train):
    X_train_G.append(cv2.cvtColor(X_train_o[i], cv2.COLOR_RGB2GRAY).reshape(32,32,1))
for i in range(n_validation):
    X_valid_G.append(cv2.cvtColor(X_valid_o[i], cv2.COLOR_RGB2GRAY).reshape(32,32,1))
for i in range(n_test):
    X_test_G.append(cv2.cvtColor(X_test_o[i], cv2.COLOR_RGB2GRAY).reshape(32,32,1))

### Normalize
X_train = np.array(X_train_G[:])/255
X_valid = np.array(X_valid_G[:])/255
X_test = np.array(X_test_G[:])/255

import tensorflow as tf

learning_rate = 0.00001
epochs = 30
batch_size = 256

test_valid_size = 512

#n_classes = 43
dropout = 0.75  # Dropout, probability to keep units

weights = {
    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 6])),
    'wc2': tf.Variable(tf.random_normal([5, 5, 6, 16])),
    'wc3': tf.Variable(tf.random_normal([5, 5, 16, 256])),
    'wd1': tf.Variable(tf.random_normal([5*5*16, 400])),
    'wd2': tf.Variable(tf.random_normal([400, 120])),
    'wd3': tf.Variable(tf.random_normal([120, 84])),
    'out': tf.Variable(tf.random_normal([84, n_classes]))}

biases = {
    'bc1': tf.Variable(tf.random_normal([6])),
    'bc2': tf.Variable(tf.random_normal([16])),
    'bc3': tf.Variable(tf.random_normal([256])),
    'bd1': tf.Variable(tf.random_normal([400])),
    'bd2': tf.Variable(tf.random_normal([120])),
    'bd3': tf.Variable(tf.random_normal([84])),
    'out': tf.Variable(tf.random_normal([n_classes]))}

def LeNet(x, weights, biases, dropout):
    # Layer 1 - 32*32*1 to 28*28*6 pooling to 14*14*6
    conv1 = conv2d(x, weights['wc1'], biases['bc1'])
    conv1 = tf.nn.relu(conv1)
    conv1 = tf.nn.dropout(conv1, dropout)
    conv1 = maxpool2d(conv1, k=2)

    # Layer 2 - 14*14*6 to 10*10*16 pooling to 5*5*16
    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])
    conv2 = tf.nn.relu(conv2)
    conv2 = tf.nn.dropout(conv2, dropout)
    conv2 = maxpool2d(conv2, k=2)

    # Fully connected layer - flatten - 5*5*16 to 400
    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])
    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])
    fc1 = tf.nn.relu(fc1)
    fc1 = tf.nn.dropout(fc1, dropout)

    # Fully connected layer - 400 to 120
    fc2 = tf.add(tf.matmul(fc1, weights['wd2']), biases['bd2'])
    fc2 = tf.nn.relu(fc2)
    fc2 = tf.nn.dropout(fc2, dropout)

    # Fully connected layer - 120 to 84
    fc3 = tf.add(tf.matmul(fc2, weights['wd3']), biases['bd3'])
    fc3 = tf.nn.relu(fc3)
    fc3 = tf.nn.dropout(fc3, dropout)

    # Output Layer - class prediction - 84 to 43
    out = tf.add(tf.matmul(fc3, weights['out']), biases['out'])

    return out

def conv2d(x, W, b, strides=1, pad='VALID'):
    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding=pad)
    x = tf.nn.bias_add(x, b)
    return x

def avgpool2d(x, k=2, pad='VALID'):
    return tf.nn.avg_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding=pad)

def maxpool2d(x, k=2, pad='VALID'):
    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding=pad)

def next_batch(batch_size,total_size,index):
    if (index+1)*batch_size < total_size:
        out_start = index*batch_size
        out_final = out_start+batch_size
    else:
        out_start = index*batch_size
        out_final = total_size
    return out_start, out_final

def onehot(val):
    # A simple one-hot encoding for single-digit integers
    out = []
    for i in range(len(val)):
        onehot = np.zeros(n_classes)
        onehot[val[i]] = 1.
        out.append(onehot)
    return out

# tf Graph input
x = tf.placeholder(tf.float32, [None, 32, 32, 1])
y = tf.placeholder(tf.float32, [None, n_classes])
keep_prob = tf.placeholder(tf.float32)

# Model
# logits = conv_net(x, weights, biases, keep_prob)
logits = LeNet(x, weights, biases, keep_prob)

# Define loss and optimizer
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))
optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)

# Accuracy
correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

# Initializing the variables
init = tf.global_variables_initializer()

config = tf.ConfigProto()
config.gpu_options.allow_growth = True

with tf.Session(config=config) as sess:
    sess.run(init)

    for epoch in range(epochs):
        for batch in range(n_train//batch_size):
            batch_start, batch_final = next_batch(batch_size, n_train, batch)
            batch_x, batch_y = X_train[batch_start:batch_final], onehot(y_train[batch_start:batch_final])
            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})

            # Calculate batch loss and accuracy
            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.})
            valid_start = np.random.randint(0, n_validation-test_valid_size)
            valid_acc = sess.run(accuracy, feed_dict={
                x: X_valid[valid_start:valid_start+test_valid_size],
                y: onehot(y_valid[valid_start:valid_start+test_valid_size]),
                keep_prob: 1.})

            print('Epoch {:>2}, Batch {:>3} - Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(
                epoch + 1,
                batch + 1,
                loss,
                valid_acc))

    # Calculate Test Accuracy
    test_acc = sess.run(accuracy, feed_dict={
        x: X_valid[:test_valid_size],
        y: onehot(y_valid[:test_valid_size]),
        keep_prob: 1.})
    print('Testing Accuracy: {}'.format(test_acc))
